{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.io\n",
    "from skimage.transform import resize\n",
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "from tqdm import tqdm\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from sklearn.utils import class_weight, shuffle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, GlobalMaxPooling2D, BatchNormalization, Input, Conv2D\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import metrics\n",
    "from keras.optimizers import Adam \n",
    "from keras import backend as K\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(42)\n",
    "np.random.seed(42)\n",
    "ia.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.5.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow\n",
    "tensorflow.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = 512\n",
    "path_to_train = 'data/train/'\n",
    "data = pd.read_csv('data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_info = []\n",
    "for name, labels in zip(data['Id'], data['Target'].str.split(' ')):\n",
    "    train_dataset_info.append({\n",
    "        'path':os.path.join(path_to_train, name),\n",
    "        'labels':np.array([int(label) for label in labels])})\n",
    "train_dataset_info = np.array(train_dataset_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_generator:\n",
    "    \n",
    "    def create_train(dataset_info, batch_size, shape, augument=True):\n",
    "        assert shape[2] == 3\n",
    "        while True:\n",
    "            dataset_info = shuffle(dataset_info)\n",
    "            for start in range(0, len(dataset_info), batch_size):\n",
    "                end = min(start + batch_size, len(dataset_info))\n",
    "                batch_images = []\n",
    "                X_train_batch = dataset_info[start:end]\n",
    "                batch_labels = np.zeros((len(X_train_batch), 28))\n",
    "                for i in range(len(X_train_batch)):\n",
    "                    image = data_generator.load_image(\n",
    "                        X_train_batch[i]['path'], shape)   \n",
    "                    if augument:\n",
    "                        image = data_generator.augment(image)\n",
    "                    batch_images.append(image/255.)\n",
    "                    batch_labels[i][X_train_batch[i]['labels']] = 1\n",
    "                yield np.array(batch_images, np.float32), batch_labels\n",
    "\n",
    "    def load_image(path, shape):\n",
    "        image_red_ch = Image.open(path+'_red.png')\n",
    "        image_yellow_ch = Image.open(path+'_yellow.png')\n",
    "        image_green_ch = Image.open(path+'_green.png')\n",
    "        image_blue_ch = Image.open(path+'_blue.png')\n",
    "        image = np.stack((\n",
    "        np.array(image_red_ch), \n",
    "        np.array(image_green_ch), \n",
    "        np.array(image_blue_ch)), -1)\n",
    "        image = cv2.resize(image, (shape[0], shape[1]))\n",
    "        return image\n",
    "\n",
    "    def augment(image):\n",
    "        augment_img = iaa.Sequential([\n",
    "            iaa.OneOf([\n",
    "                iaa.Affine(rotate=0),\n",
    "                iaa.Affine(rotate=90),\n",
    "                iaa.Affine(rotate=180),\n",
    "                iaa.Affine(rotate=270),\n",
    "                iaa.Fliplr(0.5),\n",
    "                iaa.Flipud(0.5),\n",
    "                iaa.Affine(\n",
    "                        scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)},\n",
    "                        translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)},\n",
    "                        rotate=(-180, 180),\n",
    "                        shear=(-8, 8)\n",
    "                    )\n",
    "            ])], random_order=True)\n",
    "\n",
    "        image_aug = augment_img.augment_image(image)\n",
    "        return image_aug\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape, n_out):\n",
    "    input_tensor = Input(shape=input_shape)\n",
    "    base_model = InceptionV3(include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "    bn = BatchNormalization()(input_tensor)\n",
    "    x = base_model(bn)\n",
    "    x = Conv2D(32, kernel_size=(1, 1), activation='relu')(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    output = Dense(n_out, activation='sigmoid')(x)\n",
    "    model = Model(input_tensor, output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fbeta_score_macro(y_true, y_pred, beta=1, threshold=0.2):\n",
    "\n",
    "    y_true = K.cast(y_true, 'float')\n",
    "    y_pred = K.cast(K.greater(K.cast(y_pred, 'float'), threshold), 'float')\n",
    "\n",
    "    tp = K.sum(y_true * y_pred, axis=0)\n",
    "    fp = K.sum((1 - y_true) * y_pred, axis=0)\n",
    "    fn = K.sum(y_true * (1 - y_pred), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = (1 + beta ** 2) * p * r / ((beta ** 2) * p + r + K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "\n",
    "    return K.mean(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create callbacks list\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "checkpoint = ModelCheckpoint('data/InceptionV3.h5', monitor='val_loss', verbose=1, save_best_only=True, \n",
    "                             mode='min', save_weights_only = True)\n",
    "\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, mode='auto', epsilon=0.0001)\n",
    "early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=6)\n",
    "callbacks_list = [checkpoint, early, reduceLROnPlat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train, valid\n",
    "indexes = np.arange(train_dataset_info.shape[0])\n",
    "np.random.shuffle(indexes)\n",
    "train_indexes, valid_indexes = train_test_split(indexes, test_size=0.15, random_state=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train and valid datagens\n",
    "train_generator = data_generator.create_train(train_dataset_info[train_indexes], batch_size, (SIZE, SIZE, 3), augument=True)\n",
    "validation_generator = data_generator.create_train(train_dataset_info[valid_indexes], 32, (SIZE, SIZE, 3), augument=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# warm up model\n",
    "model = create_model(input_shape=(SIZE, SIZE, 3), n_out=28)\n",
    "\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "model.layers[-1].trainable = True\n",
    "model.layers[-2].trainable = True\n",
    "model.layers[-3].trainable = True\n",
    "model.layers[-4].trainable = True\n",
    "model.layers[-5].trainable = True\n",
    "model.layers[-6].trainable = True\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(1e-03), metrics=['acc', fbeta_score_macro])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "826/826 [==============================] - 471s 570ms/step - loss: 0.1795 - acc: 0.9407 - fbeta_score_macro: 0.0627 - val_loss: 0.2202 - val_acc: 0.9383 - val_fbeta_score_macro: 0.0382\n",
      "Epoch 2/5\n",
      "826/826 [==============================] - 434s 525ms/step - loss: 0.1684 - acc: 0.9431 - fbeta_score_macro: 0.0690 - val_loss: 0.3157 - val_acc: 0.9263 - val_fbeta_score_macro: 0.0340\n",
      "Epoch 3/5\n",
      "826/826 [==============================] - 430s 521ms/step - loss: 0.1657 - acc: 0.9437 - fbeta_score_macro: 0.0783 - val_loss: 0.3542 - val_acc: 0.9260 - val_fbeta_score_macro: 0.0316\n",
      "Epoch 4/5\n",
      "826/826 [==============================] - 430s 521ms/step - loss: 0.1651 - acc: 0.9439 - fbeta_score_macro: 0.0827 - val_loss: 0.2560 - val_acc: 0.9252 - val_fbeta_score_macro: 0.0419\n",
      "Epoch 5/5\n",
      "826/826 [==============================] - 417s 504ms/step - loss: 0.1645 - acc: 0.9441 - fbeta_score_macro: 0.0842 - val_loss: 0.3414 - val_acc: 0.9181 - val_fbeta_score_macro: 0.0350\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0590604710>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=np.ceil(float(len(train_indexes)) / float(batch_size)),\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=np.ceil(float(len(valid_indexes)) / float(batch_size)),\n",
    "    epochs=5, \n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 512, 512, 3)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_95 (Batc (None, 512, 512, 3)       12        \n",
      "_________________________________________________________________\n",
      "inception_v3 (Model)         (None, 14, 14, 2048)      21802784  \n",
      "_________________________________________________________________\n",
      "conv2d_95 (Conv2D)           (None, 14, 14, 32)        65568     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              6423552   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 28)                28700     \n",
      "=================================================================\n",
      "Total params: 28,320,616\n",
      "Trainable params: 28,286,178\n",
      "Non-trainable params: 34,438\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# train all layers\n",
    "for layer in model.layers:\n",
    "    layer.trainable = True\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(lr=1e-3), metrics=['accuracy', fbeta_score_macro])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "826/826 [==============================] - 870s 1s/step - loss: 0.1667 - acc: 0.9436 - fbeta_score_macro: 0.0593 - val_loss: 0.1726 - val_acc: 0.9438 - val_fbeta_score_macro: 0.0624\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.17263, saving model to data/InceptionV3.h5\n",
      "Epoch 2/100\n",
      "826/826 [==============================] - 869s 1s/step - loss: 0.1560 - acc: 0.9484 - fbeta_score_macro: 0.0926 - val_loss: 0.1531 - val_acc: 0.9508 - val_fbeta_score_macro: 0.1044\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.17263 to 0.15310, saving model to data/InceptionV3.h5\n",
      "Epoch 3/100\n",
      "826/826 [==============================] - 867s 1s/step - loss: 0.1477 - acc: 0.9503 - fbeta_score_macro: 0.1287 - val_loss: 0.1639 - val_acc: 0.9457 - val_fbeta_score_macro: 0.1146\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.15310\n",
      "Epoch 4/100\n",
      "826/826 [==============================] - 868s 1s/step - loss: 0.1424 - acc: 0.9509 - fbeta_score_macro: 0.1516 - val_loss: 0.1845 - val_acc: 0.9370 - val_fbeta_score_macro: 0.0787\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.15310\n",
      "Epoch 5/100\n",
      "826/826 [==============================] - 868s 1s/step - loss: 0.1371 - acc: 0.9526 - fbeta_score_macro: 0.1691 - val_loss: 0.1433 - val_acc: 0.9503 - val_fbeta_score_macro: 0.1551\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.15310 to 0.14326, saving model to data/InceptionV3.h5\n",
      "Epoch 6/100\n",
      "826/826 [==============================] - 869s 1s/step - loss: 0.1312 - acc: 0.9544 - fbeta_score_macro: 0.1992 - val_loss: 0.1331 - val_acc: 0.9535 - val_fbeta_score_macro: 0.2022\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.14326 to 0.13307, saving model to data/InceptionV3.h5\n",
      "Epoch 7/100\n",
      "826/826 [==============================] - 869s 1s/step - loss: 0.1250 - acc: 0.9567 - fbeta_score_macro: 0.2233 - val_loss: 0.1292 - val_acc: 0.9550 - val_fbeta_score_macro: 0.2095\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.13307 to 0.12917, saving model to data/InceptionV3.h5\n",
      "Epoch 8/100\n",
      "826/826 [==============================] - 869s 1s/step - loss: 0.1220 - acc: 0.9576 - fbeta_score_macro: 0.2344 - val_loss: 0.1513 - val_acc: 0.9464 - val_fbeta_score_macro: 0.1909\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.12917\n",
      "Epoch 9/100\n",
      "826/826 [==============================] - 869s 1s/step - loss: 0.1178 - acc: 0.9593 - fbeta_score_macro: 0.2500 - val_loss: 0.1280 - val_acc: 0.9544 - val_fbeta_score_macro: 0.2196\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.12917 to 0.12800, saving model to data/InceptionV3.h5\n",
      "Epoch 10/100\n",
      "826/826 [==============================] - 869s 1s/step - loss: 0.1140 - acc: 0.9604 - fbeta_score_macro: 0.2605 - val_loss: 0.1146 - val_acc: 0.9593 - val_fbeta_score_macro: 0.2573\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.12800 to 0.11456, saving model to data/InceptionV3.h5\n",
      "Epoch 11/100\n",
      "826/826 [==============================] - 867s 1s/step - loss: 0.1118 - acc: 0.9615 - fbeta_score_macro: 0.2672 - val_loss: 0.1392 - val_acc: 0.9537 - val_fbeta_score_macro: 0.2145\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.11456\n",
      "Epoch 12/100\n",
      "826/826 [==============================] - 859s 1s/step - loss: 0.1108 - acc: 0.9619 - fbeta_score_macro: 0.2701 - val_loss: 0.1172 - val_acc: 0.9602 - val_fbeta_score_macro: 0.2431\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.11456\n",
      "Epoch 13/100\n",
      "826/826 [==============================] - 855s 1s/step - loss: 0.1086 - acc: 0.9623 - fbeta_score_macro: 0.2798 - val_loss: 0.1142 - val_acc: 0.9590 - val_fbeta_score_macro: 0.2610\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.11456 to 0.11417, saving model to data/InceptionV3.h5\n",
      "Epoch 14/100\n",
      "826/826 [==============================] - 865s 1s/step - loss: 0.1068 - acc: 0.9631 - fbeta_score_macro: 0.2861 - val_loss: 0.1337 - val_acc: 0.9531 - val_fbeta_score_macro: 0.2412\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.11417\n",
      "Epoch 15/100\n",
      "826/826 [==============================] - 857s 1s/step - loss: 0.1042 - acc: 0.9640 - fbeta_score_macro: 0.2926 - val_loss: 0.1099 - val_acc: 0.9627 - val_fbeta_score_macro: 0.2807\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.11417 to 0.10989, saving model to data/InceptionV3.h5\n",
      "Epoch 16/100\n",
      "826/826 [==============================] - 864s 1s/step - loss: 0.1035 - acc: 0.9642 - fbeta_score_macro: 0.2949 - val_loss: 0.1006 - val_acc: 0.9647 - val_fbeta_score_macro: 0.2976\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.10989 to 0.10056, saving model to data/InceptionV3.h5\n",
      "Epoch 17/100\n",
      "826/826 [==============================] - 861s 1s/step - loss: 0.1026 - acc: 0.9644 - fbeta_score_macro: 0.2958 - val_loss: 0.1192 - val_acc: 0.9596 - val_fbeta_score_macro: 0.2640\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.10056\n",
      "Epoch 18/100\n",
      "826/826 [==============================] - 858s 1s/step - loss: 0.1008 - acc: 0.9653 - fbeta_score_macro: 0.3024 - val_loss: 0.0991 - val_acc: 0.9659 - val_fbeta_score_macro: 0.3016\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.10056 to 0.09913, saving model to data/InceptionV3.h5\n",
      "Epoch 19/100\n",
      "826/826 [==============================] - 861s 1s/step - loss: 0.0995 - acc: 0.9655 - fbeta_score_macro: 0.3093 - val_loss: 0.1073 - val_acc: 0.9629 - val_fbeta_score_macro: 0.2753\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.09913\n",
      "Epoch 20/100\n",
      "826/826 [==============================] - 861s 1s/step - loss: 0.0985 - acc: 0.9658 - fbeta_score_macro: 0.3075 - val_loss: 0.1032 - val_acc: 0.9640 - val_fbeta_score_macro: 0.2907\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.09913\n",
      "Epoch 21/100\n",
      "826/826 [==============================] - 868s 1s/step - loss: 0.0974 - acc: 0.9664 - fbeta_score_macro: 0.3138 - val_loss: 0.0964 - val_acc: 0.9661 - val_fbeta_score_macro: 0.3132\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.09913 to 0.09635, saving model to data/InceptionV3.h5\n",
      "Epoch 22/100\n",
      "826/826 [==============================] - 870s 1s/step - loss: 0.0961 - acc: 0.9666 - fbeta_score_macro: 0.3173 - val_loss: 0.0968 - val_acc: 0.9669 - val_fbeta_score_macro: 0.3126\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.09635\n",
      "Epoch 23/100\n",
      "826/826 [==============================] - 857s 1s/step - loss: 0.0961 - acc: 0.9666 - fbeta_score_macro: 0.3192 - val_loss: 0.1086 - val_acc: 0.9630 - val_fbeta_score_macro: 0.2936\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.09635\n",
      "Epoch 24/100\n",
      "826/826 [==============================] - 861s 1s/step - loss: 0.0951 - acc: 0.9670 - fbeta_score_macro: 0.3206 - val_loss: 0.1014 - val_acc: 0.9648 - val_fbeta_score_macro: 0.3032\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.09635\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 25/100\n",
      "826/826 [==============================] - 864s 1s/step - loss: 0.0900 - acc: 0.9688 - fbeta_score_macro: 0.3329 - val_loss: 0.0862 - val_acc: 0.9695 - val_fbeta_score_macro: 0.3415\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.09635 to 0.08619, saving model to data/InceptionV3.h5\n",
      "Epoch 26/100\n",
      "826/826 [==============================] - 864s 1s/step - loss: 0.0875 - acc: 0.9694 - fbeta_score_macro: 0.3409 - val_loss: 0.0851 - val_acc: 0.9702 - val_fbeta_score_macro: 0.3403\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.08619 to 0.08506, saving model to data/InceptionV3.h5\n",
      "Epoch 27/100\n",
      "826/826 [==============================] - 868s 1s/step - loss: 0.0866 - acc: 0.9697 - fbeta_score_macro: 0.3431 - val_loss: 0.0849 - val_acc: 0.9700 - val_fbeta_score_macro: 0.3450\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.08506 to 0.08491, saving model to data/InceptionV3.h5\n",
      "Epoch 28/100\n",
      "826/826 [==============================] - 869s 1s/step - loss: 0.0855 - acc: 0.9702 - fbeta_score_macro: 0.3423 - val_loss: 0.0847 - val_acc: 0.9702 - val_fbeta_score_macro: 0.3468\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.08491 to 0.08466, saving model to data/InceptionV3.h5\n",
      "Epoch 29/100\n",
      "826/826 [==============================] - 867s 1s/step - loss: 0.0848 - acc: 0.9703 - fbeta_score_macro: 0.3472 - val_loss: 0.0841 - val_acc: 0.9702 - val_fbeta_score_macro: 0.3507\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.08466 to 0.08411, saving model to data/InceptionV3.h5\n",
      "Epoch 30/100\n",
      "826/826 [==============================] - 871s 1s/step - loss: 0.0839 - acc: 0.9704 - fbeta_score_macro: 0.3493 - val_loss: 0.0838 - val_acc: 0.9705 - val_fbeta_score_macro: 0.3485\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.08411 to 0.08381, saving model to data/InceptionV3.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/100\n",
      "826/826 [==============================] - 869s 1s/step - loss: 0.0839 - acc: 0.9706 - fbeta_score_macro: 0.3506 - val_loss: 0.0840 - val_acc: 0.9702 - val_fbeta_score_macro: 0.3484\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.08381\n",
      "Epoch 32/100\n",
      "826/826 [==============================] - 867s 1s/step - loss: 0.0835 - acc: 0.9707 - fbeta_score_macro: 0.3505 - val_loss: 0.0838 - val_acc: 0.9701 - val_fbeta_score_macro: 0.3560\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.08381 to 0.08380, saving model to data/InceptionV3.h5\n",
      "Epoch 33/100\n",
      "826/826 [==============================] - 868s 1s/step - loss: 0.0829 - acc: 0.9709 - fbeta_score_macro: 0.3518 - val_loss: 0.0825 - val_acc: 0.9710 - val_fbeta_score_macro: 0.3540\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.08380 to 0.08252, saving model to data/InceptionV3.h5\n",
      "Epoch 34/100\n",
      "826/826 [==============================] - 868s 1s/step - loss: 0.0823 - acc: 0.9712 - fbeta_score_macro: 0.3517 - val_loss: 0.0829 - val_acc: 0.9708 - val_fbeta_score_macro: 0.3559\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.08252\n",
      "Epoch 35/100\n",
      "826/826 [==============================] - 868s 1s/step - loss: 0.0824 - acc: 0.9711 - fbeta_score_macro: 0.3521 - val_loss: 0.0827 - val_acc: 0.9709 - val_fbeta_score_macro: 0.3539\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.08252\n",
      "Epoch 36/100\n",
      "826/826 [==============================] - 865s 1s/step - loss: 0.0820 - acc: 0.9713 - fbeta_score_macro: 0.3546 - val_loss: 0.0830 - val_acc: 0.9710 - val_fbeta_score_macro: 0.3550\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.08252\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 37/100\n",
      "826/826 [==============================] - 865s 1s/step - loss: 0.0812 - acc: 0.9714 - fbeta_score_macro: 0.3564 - val_loss: 0.0819 - val_acc: 0.9712 - val_fbeta_score_macro: 0.3560\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.08252 to 0.08190, saving model to data/InceptionV3.h5\n",
      "Epoch 38/100\n",
      "826/826 [==============================] - 867s 1s/step - loss: 0.0803 - acc: 0.9718 - fbeta_score_macro: 0.3583 - val_loss: 0.0822 - val_acc: 0.9711 - val_fbeta_score_macro: 0.3552\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.08190\n",
      "Epoch 39/100\n",
      "826/826 [==============================] - 866s 1s/step - loss: 0.0803 - acc: 0.9717 - fbeta_score_macro: 0.3574 - val_loss: 0.0821 - val_acc: 0.9711 - val_fbeta_score_macro: 0.3580\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.08190\n",
      "Epoch 40/100\n",
      "826/826 [==============================] - 866s 1s/step - loss: 0.0803 - acc: 0.9719 - fbeta_score_macro: 0.3582 - val_loss: 0.0814 - val_acc: 0.9715 - val_fbeta_score_macro: 0.3571\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.08190 to 0.08137, saving model to data/InceptionV3.h5\n",
      "Epoch 41/100\n",
      "826/826 [==============================] - 871s 1s/step - loss: 0.0804 - acc: 0.9717 - fbeta_score_macro: 0.3579 - val_loss: 0.0818 - val_acc: 0.9713 - val_fbeta_score_macro: 0.3546\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.08137\n",
      "Epoch 42/100\n",
      "826/826 [==============================] - 867s 1s/step - loss: 0.0802 - acc: 0.9719 - fbeta_score_macro: 0.3598 - val_loss: 0.0822 - val_acc: 0.9711 - val_fbeta_score_macro: 0.3556\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.08137\n",
      "Epoch 43/100\n",
      "826/826 [==============================] - 871s 1s/step - loss: 0.0805 - acc: 0.9717 - fbeta_score_macro: 0.3593 - val_loss: 0.0814 - val_acc: 0.9713 - val_fbeta_score_macro: 0.3578\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.08137\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 44/100\n",
      "826/826 [==============================] - 873s 1s/step - loss: 0.0803 - acc: 0.9717 - fbeta_score_macro: 0.3566 - val_loss: 0.0819 - val_acc: 0.9713 - val_fbeta_score_macro: 0.3591\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.08137\n",
      "Epoch 45/100\n",
      "826/826 [==============================] - 872s 1s/step - loss: 0.0799 - acc: 0.9719 - fbeta_score_macro: 0.3619 - val_loss: 0.0814 - val_acc: 0.9713 - val_fbeta_score_macro: 0.3607\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.08137\n",
      "Epoch 46/100\n",
      "826/826 [==============================] - 873s 1s/step - loss: 0.0800 - acc: 0.9719 - fbeta_score_macro: 0.3576 - val_loss: 0.0819 - val_acc: 0.9713 - val_fbeta_score_macro: 0.3548\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.08137\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f01512450b8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=np.ceil(float(len(train_indexes)) / float(batch_size)),\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=np.ceil(float(len(valid_indexes)) / float(batch_size)),\n",
    "    epochs=epochs, \n",
    "    verbose=1,\n",
    "    callbacks=callbacks_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11702/11702 [09:31<00:00, 20.47it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create submit\n",
    "submit = pd.read_csv('data/sample_submission.csv')\n",
    "predicted = []\n",
    "draw_predict = []\n",
    "model.load_weights('data/InceptionV3.h5')\n",
    "for name in tqdm(submit['Id']):\n",
    "    path = os.path.join('data/test/', name)\n",
    "    image = data_generator.load_image(path, (SIZE,SIZE,3))/255.\n",
    "    score_predict = model.predict(image[np.newaxis])[0]\n",
    "    draw_predict.append(score_predict)\n",
    "    label_predict = np.arange(28)[score_predict >= 0.2]\n",
    "    str_predict_label = ' '.join(str(l) for l in label_predict)\n",
    "    predicted.append(str_predict_label)\n",
    "\n",
    "submit['Predicted'] = predicted\n",
    "np.save('data/draw_predict_InceptionV3_512.npy', score_predict)\n",
    "submit.to_csv('data/submit_InceptionV3_512.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00008af0-bad0-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000a892-bacf-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>5 25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0006faa6-bac7-11e8-b2b7-ac1f6b6435d0</td>\n",
       "      <td>0 5 25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0008baca-bad7-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>0 25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000cce7e-bad4-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>0 23 25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Id Predicted\n",
       "0  00008af0-bad0-11e8-b2b8-ac1f6b6435d0         2\n",
       "1  0000a892-bacf-11e8-b2b8-ac1f6b6435d0      5 25\n",
       "2  0006faa6-bac7-11e8-b2b7-ac1f6b6435d0    0 5 25\n",
       "3  0008baca-bad7-11e8-b2b9-ac1f6b6435d0      0 25\n",
       "4  000cce7e-bad4-11e8-b2b8-ac1f6b6435d0   0 23 25"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/submit_InceptionV3_512.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
